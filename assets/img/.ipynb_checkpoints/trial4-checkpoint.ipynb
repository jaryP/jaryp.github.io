{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.prune import l1_unstructured\n",
    "from torch.nn.utils.prune import remove\n",
    "\n",
    "use_cuda = True\n",
    "use_cuda = use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(1, 64, 3)   #CIFAR10 --> (3,64,3)\n",
    "        #self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.fc1 = nn.Linear(784, 128)   #MNIST_batches --> 3200       ; #CIFAR10 --> (4608, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.importances = [(k, torch.zeros_like(p).to(p.device)) for k, p in self.named_parameters()]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x =  F.max_pool2d(F.relu(self.conv1(x)), (2,2)) \n",
    "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1,10)\n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        # reinitialize network\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()\n",
    "\n",
    "        # clear importance\n",
    "        for _,imp in self.importances:\n",
    "            imp.fill_(0.)\n",
    "\n",
    "#MODEL\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "modello = deepcopy(model)\n",
    "pre_w = deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "lr = 0.01\n",
    "epochs = 3\n",
    "batch_size = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim1 = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "optim2 = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "name = 'MNIST'\n",
    "m = getattr(datasets, name)\n",
    "\n",
    "ds_train = m(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "ds_test = m(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(ds_test,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_iters = int(np.ceil(train_loader.dataset.data.shape[0] * epochs / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 180/1800 iterations done.     Loss: 1.775\n",
      "Training 360/1800 iterations done.     Loss: 0.942\n",
      "Training 540/1800 iterations done.     Loss: 0.632\n",
      "Training 720/1800 iterations done.     Loss: 0.506\n",
      "Training 900/1800 iterations done.     Loss: 0.432\n",
      "Training 1080/1800 iterations done.     Loss: 0.400\n",
      "Training 1260/1800 iterations done.     Loss: 0.376\n",
      "Training 1440/1800 iterations done.     Loss: 0.347\n",
      "Training 1620/1800 iterations done.     Loss: 0.331\n",
      "Training 1800/1800 iterations done.     Loss: 0.320\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "running_loss = 0.0\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device); labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optim2.zero_grad()\n",
    "        loss.backward()\n",
    "        optim2.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if idx % 180 == 0:\n",
    "                print(f'\\rTraining {idx}/{num_iters} iterations done.     Loss: {running_loss /180:.3f}')#, end='')\n",
    "                running_loss = 0.0\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  tensor(0.2565)  --- test_acc:  tensor(0.9300)\n",
      "test_loss :  tensor(0.4951)  --- test_acc:  tensor(0.8800)\n",
      "test_loss :  tensor(0.3804)  --- test_acc:  tensor(0.8900)\n",
      "test_loss :  tensor(0.4049)  --- test_acc:  tensor(0.9000)\n",
      "test_loss :  tensor(0.2202)  --- test_acc:  tensor(0.9300)\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "with torch.no_grad():\n",
    "    for j, (inputs,labels) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(inputs)\n",
    "        lost = F.cross_entropy(out, labels) \n",
    "\n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "        accuracy.append(acc)\n",
    "\n",
    "        if j % 20 == 0:\n",
    "            print('test_loss : ', lost, ' --- test_acc: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate IMPORTANCES of weights (Fisher Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_loader.dataset.data\n",
    "\n",
    "accumulators = [torch.zeros_like(p, device=device) for p in model.parameters()]\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    for accum, v in zip(accumulators, model.parameters()):\n",
    "        accum.add_(v.grad.square())\n",
    "\n",
    "for accum, (_,imp) in zip(accumulators, model.importances):\n",
    "    imp.add_(accum / len(inputs))\n",
    "\n",
    "\n",
    "\n",
    "new_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking: threshold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 5e-06\n",
      "fc1.weight\n",
      "Number of weights that changes much:  3157\n",
      "Number of weights that changes less:  97195\n",
      "We can prune the 96.8541 % of the weights\n",
      "fc1.bias\n",
      "Number of weights that changes much:  51\n",
      "Number of weights that changes less:  77\n",
      "We can prune the 60.1562 % of the weights\n",
      "fc2.weight\n",
      "Number of weights that changes much:  1062\n",
      "Number of weights that changes less:  218\n",
      "We can prune the 17.0312 % of the weights\n",
      "fc2.bias\n",
      "Number of weights that changes much:  10\n",
      "Number of weights that changes less:  0\n",
      "We can prune the 0.0 % of the weights\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.000005\n",
    "new_imp = model.importances\n",
    "masks = {}\n",
    "perc = []\n",
    "print('Threshold', thresh)\n",
    "for k in range(len(new_imp)):\n",
    "    bool_tensor = new_imp[k][1] > thresh\n",
    "    mask = torch.where(bool_tensor == True, 1., 0.)\n",
    "    masks[tuple(new_imp[k][0].split(\".\")) ] = mask\n",
    "    print(new_imp[k][0])\n",
    "    true_values = bool_tensor.masked_select(bool_tensor == True)\n",
    "    false_values = bool_tensor.masked_select(bool_tensor == False)\n",
    "    true_num = len(true_values)\n",
    "    false_num = len(false_values)\n",
    "    print('Number of weights that changes much: ',true_num)\n",
    "    print('Number of weights that changes less: ',false_num)\n",
    "    perc.append(false_num/(false_num+true_num))\n",
    "    print('We can prune the', round(false_num/(false_num+true_num)*100, 4), '% of the weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pr, (lay,imp) in zip(perc,new_model.importances):\n",
    "        name =  tuple(lay.split(\".\"))\n",
    "        module = getattr(new_model, name[0])\n",
    "        l1_unstructured(module, name[1],  amount = pr, importance_scores=imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking: percentile method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lay,imp in new_model.importances:\n",
    "        name =  tuple(lay.split(\".\"))\n",
    "        module = getattr(new_model, name[0])\n",
    "        l1_unstructured(module, name[1],  amount = 0.3, importance_scores=imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "E' meglio un pruning locale o globale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make pruning permanent this function reassigns the parameter weight to the model parameters, in its pruned version.\n",
    "def remove_params(model):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            try:\n",
    "                remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            try:\n",
    "                remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "    return model\n",
    "\n",
    "pruned_model = remove_params(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-training the pruned_model\n",
    "\n",
    "Ho bisogno della penalty da sommare alla loss?\n",
    "\n",
    "Ho bisogno di lambda per pesare l'importanza dell'EWC?\n",
    "\n",
    "Utilizzo lo stesso train_dataset o faccio lo shuffle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 180/1800 iterations done.     Loss: 1.053\n",
      "Training 360/1800 iterations done.     Loss: 1.042\n",
      "Training 540/1800 iterations done.     Loss: 1.031\n",
      "Training 720/1800 iterations done.     Loss: 1.040\n",
      "Training 900/1800 iterations done.     Loss: 1.042\n",
      "Training 1080/1800 iterations done.     Loss: 1.043\n",
      "Training 1260/1800 iterations done.     Loss: 1.041\n",
      "Training 1440/1800 iterations done.     Loss: 1.032\n",
      "Training 1620/1800 iterations done.     Loss: 1.059\n",
      "Training 1800/1800 iterations done.     Loss: 1.026\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "running_loss = 0.0\n",
    "#lmbda = 1000\n",
    "#penalty = {}\n",
    "#star_params = [p.clone().detach() for p in pruned_model.parameters()]\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device); labels = labels.to(device)\n",
    "        \n",
    "        outputs = pruned_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #for (name,p), (_,imp), p_star in zip(pruned_model.named_parameters(), pruned_model.importances, star_params):\n",
    "        #        penalty[name] = lmbda * torch.sum(imp * torch.square(p - p_star))\n",
    "        #        loss += lmbda * torch.sum(imp * torch.square(p - p_star))  \n",
    "     \n",
    "        optim2.zero_grad()\n",
    "        loss.backward()\n",
    "        optim2.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if idx % 180 == 0:\n",
    "                print(f'\\rTraining {idx}/{num_iters} iterations done.     Loss: {running_loss /180:.3f}')#, end='')\n",
    "                running_loss = 0.0\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perché la loss non scende?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  tensor(0.7444)  --- test_acc:  tensor(0.7500)\n",
      "test_loss :  tensor(1.0868)  --- test_acc:  tensor(0.6700)\n",
      "test_loss :  tensor(1.0407)  --- test_acc:  tensor(0.6800)\n",
      "test_loss :  tensor(1.3259)  --- test_acc:  tensor(0.6500)\n",
      "test_loss :  tensor(1.1741)  --- test_acc:  tensor(0.6700)\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "with torch.no_grad():\n",
    "    for j, (inputs,labels) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = pruned_model(inputs)\n",
    "        lost = F.cross_entropy(out, labels) \n",
    "\n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "        accuracy.append(acc)\n",
    "\n",
    "        if j % 20 == 0:\n",
    "            print('test_loss : ', lost, ' --- test_acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0., -0., -0.,  ..., 0., -0., -0.],\n",
      "        [0., -0., -0.,  ..., -0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., -0., 0.,  ..., 0., -0., -0.],\n",
      "        [0., -0., 0.,  ..., 0., 0., -0.],\n",
      "        [0., -0., 0.,  ..., 0., -0., 0.]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0707,  0.0000,  0.0000,  0.0411,  0.0696,  0.0571,  0.0293,  0.0331,\n",
      "         0.0517,  0.0725,  0.0089,  0.0000,  0.0000, -0.0397,  0.0432,  0.0503,\n",
      "         0.0472,  0.0250, -0.0000, -0.0040,  0.0204,  0.0783, -0.0042,  0.0058,\n",
      "         0.0466,  0.0477,  0.0119, -0.0000, -0.0184,  0.0309,  0.0233,  0.0468,\n",
      "        -0.0263,  0.0500, -0.0036,  0.0421,  0.0000,  0.0000, -0.0051,  0.0465,\n",
      "        -0.0426, -0.0000,  0.0465,  0.0431,  0.0355,  0.0126, -0.0077, -0.0423,\n",
      "         0.0000, -0.0105,  0.0403,  0.0000,  0.0631,  0.0000, -0.0016,  0.0321,\n",
      "         0.0747,  0.0263, -0.0043, -0.0000,  0.0000, -0.0336,  0.0000, -0.0000,\n",
      "         0.0463,  0.0425,  0.0000,  0.0720,  0.0563,  0.0113,  0.0245,  0.0532,\n",
      "         0.0061,  0.0544,  0.0000, -0.0000,  0.0000, -0.0419, -0.0076, -0.0000,\n",
      "         0.0000, -0.0000,  0.0201,  0.0032,  0.0253, -0.0437,  0.0236, -0.0235,\n",
      "         0.0116,  0.0556,  0.0000, -0.0318,  0.0574,  0.0489, -0.0306,  0.0202,\n",
      "         0.0317, -0.0134,  0.0231, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0043,  0.0443,  0.0077, -0.0000,  0.0616, -0.0000, -0.0215,  0.0375,\n",
      "         0.0586, -0.0000,  0.0496,  0.0000,  0.0000,  0.0000,  0.0247,  0.0255,\n",
      "         0.0240,  0.0000, -0.0000,  0.0850, -0.0151,  0.0000,  0.0419,  0.0862],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0000, -0.0000, -0.0778,  ..., -0.0000, -0.0000, -0.0435],\n",
      "        [ 0.0000, -0.0159,  0.0762,  ...,  0.0351,  0.1049, -0.1125],\n",
      "        [-0.0914,  0.1398, -0.1003,  ...,  0.0000, -0.0363, -0.0283],\n",
      "        ...,\n",
      "        [-0.0000, -0.0000,  0.0698,  ..., -0.0000, -0.1335,  0.1127],\n",
      "        [-0.0000, -0.0000,  0.1212,  ..., -0.0000, -0.1120, -0.1615],\n",
      "        [-0.1023,  0.0237,  0.0715,  ..., -0.0000,  0.0000,  0.1398]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0000,  0.0819, -0.0000,  0.0040,  0.0961,  0.0923, -0.0425, -0.0473,\n",
      "        -0.0000, -0.0063], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(pruned_model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
